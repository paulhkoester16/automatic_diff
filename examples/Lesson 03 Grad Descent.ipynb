{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 Automatic Grad Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import automatic_diff as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent implementations usually require the user to provide both the\n",
    "function and its gradient.  For complex objective functions, this requires\n",
    "the user to compute, and code, the gradients by hand, a tedious and error prone\n",
    "exercise.  \n",
    "\n",
    "Automatic differentiation greatly simplifies the task, as the user only needs \n",
    "to provide the function, letting the system automatically fill in the derivatives. \n",
    "\n",
    "\n",
    "Consider\n",
    "$$\n",
    "    f(x) = (x - 3)^2 + 5\n",
    "$$\n",
    "\n",
    "Basic algebra tells us that the function has a global minimum at $x=3$, and the minimum \n",
    "value is $5.$\n",
    "\n",
    "Let's try to find this minima using gradient descent, starting at $x=10.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate minimizer:  2.99999985664\n",
      "Current grad step in x:  8.601599997604125e-07\n",
      "Approximate minimal value:  5.000000000000513\n",
      "Current grad step in y: 1.4335999996006876e-06\n"
     ]
    }
   ],
   "source": [
    "f_univar = lambda d: (d - 3)**2 + 5\n",
    "x, y, dy = ad.grad_descent.grad_descent(\n",
    "    np.array([10]), f_univar, max_iters=100, tol=1e-6, lr=0.6\n",
    ")\n",
    "\n",
    "print(\"Approximate minimizer: \", x.x[0])\n",
    "print(\"Current grad step in x: \", x.dx[0])\n",
    "print(\"Approximate minimal value: \", y)\n",
    "print(\"Current grad step in y:\", dy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, for the multivariable function \n",
    "$$\n",
    "    f(x_0, x_1) = (x_0 - 2)^2 + (x_1 + 3)^2 + 8,\n",
    "$$\n",
    "the minimal value occurs at $(x_0, x_1) = (2, -3)$, and the minimum value is $8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate minimizer:  [ 1.9999959  -3.00000768]\n",
      "Current grad step in x:  [2.4576e-05 4.6080e-05]\n",
      "Approximate minimal value:  8.00000000189399\n",
      "Current grad step in y: [array(4.096e-05), array(7.68e-05)]\n"
     ]
    }
   ],
   "source": [
    "f_multivar = lambda d_0, d_1: (d_0 - 2)**2 + (d_1 + 3)**2 + 8\n",
    "x, y, dy = ad.grad_descent.grad_descent(\n",
    "    np.array([10, 12]), f_multivar, max_iters=100, tol=1e-4, lr=0.6\n",
    ")\n",
    "\n",
    "print(\"Approximate minimizer: \", x.x)\n",
    "print(\"Current grad step in x: \", x.dx)\n",
    "print(\"Approximate minimal value: \", y)\n",
    "print(\"Current grad step in y:\", dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rosenbrock function is \n",
    "$$\n",
    "    f(x_0, x_1) = (a - x_0)^{2} + b(x_1 - x_0^{2})^{2}\n",
    "$$\n",
    "\n",
    "When $b \\geq 0$, the function's minimum value is zero, which occurs when both summands \n",
    "are 0, and so the minimizer is $(x_0, x_1) = (a, a^2).$\n",
    "\n",
    "Despite the fact that the function is trivial to optimize using basic algebraic logic, \n",
    "it turns out to be very challenging to optimize using numerical methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate minimizer:  [ 2. -3.]\n",
      "Current grad step in x:  [1.57286397e-09 2.94911988e-09]\n",
      "Approximate minimal value:  8.0\n",
      "Current grad step in y: [array(2.62143995e-09), array(4.9151998e-09)]\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 100\n",
    "\n",
    "f_rosenbrock = lambda d_0, d_1: (a - d_0)**2 + b(d_1 - d_0**2)**2 \n",
    "\n",
    "x, y, dy = ad.grad_descent.grad_descent(\n",
    "    np.array([10, 12]), f_multivar, max_iters=100, tol=1e-8, lr=0.6\n",
    ")\n",
    "\n",
    "print(\"Approximate minimizer: \", x.x)\n",
    "print(\"Current grad step in x: \", x.dx)\n",
    "print(\"Approximate minimal value: \", y)\n",
    "print(\"Current grad step in y:\", dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our naive gradient descent gets fooled by Rosenbrock.   \n",
    "\n",
    "Some day we might come back and improve our gradient descent algorithm, utilizing adaptive learning rates, momentum, etc.  But we have not attempted that yet.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
